{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7e9c32e-edb7-43f4-9c39-607b27249d9d",
   "metadata": {},
   "source": [
    "## Using SmartNoise Synthesizers to generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f1ccd2de-e8e9-4b58-b1dc-2d279c300e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install mbi with:\n",
      "   pip install git+https://github.com/ryan112358/private-pgm.git@01f02f17eba440f4e76c1d06fa5ee9eed0bd2bca\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/School/cs3110-final-project/.venv/lib/python3.12/site-packages/snsynth/mst/mst.py:5\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmbi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FactoredInference, Dataset, Domain\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mbi'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaternalHealthDataSet.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# MST synthesizer is used here since it took 1st place in NIST's DP syntehtic data contest\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m synth \u001b[38;5;241m=\u001b[39m \u001b[43mSynthesizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmst\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m synth\u001b[38;5;241m.\u001b[39mfit(data, preprocessor_eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     10\u001b[0m data_synth \u001b[38;5;241m=\u001b[39m synth\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m1000\u001b[39m)\n",
      "File \u001b[0;32m~/School/cs3110-final-project/.venv/lib/python3.12/site-packages/snsynth/base.py:224\u001b[0m, in \u001b[0;36mSynthesizer.create\u001b[0;34m(cls, synth, epsilon, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m synth_class \u001b[38;5;241m=\u001b[39m synth_map[synth][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    223\u001b[0m synth_module, synth_class \u001b[38;5;241m=\u001b[39m synth_class\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 224\u001b[0m synth_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msynth_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfromlist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43msynth_class\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m synth_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(synth_module, synth_class)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m synth_class(epsilon\u001b[38;5;241m=\u001b[39mepsilon, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/School/cs3110-final-project/.venv/lib/python3.12/site-packages/snsynth/mst/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmst\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MSTSynthesizer\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSTSynthesizer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/School/cs3110-final-project/.venv/lib/python3.12/site-packages/snsynth/mst/mst.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install mbi with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m   pip install git+https://github.com/ryan112358/private-pgm.git@01f02f17eba440f4e76c1d06fa5ee9eed0bd2bca\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdisjoint_set\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DisjointSet\n",
      "\u001b[0;31mImportError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from snsynth import Synthesizer # TODO: GETTING WIERD ERROR HERE, ANYONE ELSE?\n",
    "import pandas as pd\n",
    "\n",
    "# TODO: NOTE: ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
    "            # smartnoise-synth 1.0.4 requires opacus<0.15.0,>=0.14.0, but you have opacus 1.5.2 which is incompatible.\n",
    "\n",
    "data = pd.read_csv(\"maternalHealthDataSet.csv\")\n",
    "\n",
    "\n",
    "# MST synthesizer is used here since it took 1st place in NIST's DP syntehtic data contest\n",
    "synth = Synthesizer.create(\"mst\", epsilon=1.0, delta=1e-5, verbose=True)\n",
    "synth.fit(data, preprocessor_eps=1.0)\n",
    "data_synth = synth.sample(1000)\n",
    "data_synth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b816dc0-b938-4529-b040-3d3aca6e7570",
   "metadata": {},
   "source": [
    "## Justification for using a Decision Tree\n",
    "\n",
    "Showing that an MLP does not have sufficient accuracy using DP-SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a0bd9d-cea3-44d6-87c9-af1e4814ea12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jack\\OneDrive\\Desktop\\cs3110\\cs3110-final-project\\.venv\\Lib\\site-packages\\opacus\\privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jack\\OneDrive\\Desktop\\cs3110\\cs3110-final-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/150, Train Loss: 0.0872, Validation Loss: 0.0810, Validation Accuracy: 0.4877\n"
     ]
    }
   ],
   "source": [
    "from opacus import PrivacyEngine\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Define MLP\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define layers (hidden layer size = input size here)\n",
    "        self.layers = nn.Sequential(\n",
    "            # fully connected layer, 6 input to 6\n",
    "            nn.Linear(6, 6),\n",
    "            nn.ReLU(),\n",
    "            # fully connected layer, 6 to 3 output\n",
    "            nn.Linear(6, 3)\n",
    "        )\n",
    "        # handles typeErrors for Linear layers\n",
    "        self.double()\n",
    "\n",
    "    # forward propagation\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "# Create model\n",
    "model = MLP()\n",
    "\n",
    "# Model params\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "num_epochs = 150\n",
    "    \n",
    "# Define loss function & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Load data\n",
    "# Drop one-hot encoding and string label column\n",
    "health_data = pd.read_csv(\"maternalHealthDataSet.csv\").drop([\"RiskLevelStr\",\"MidRisk\",\"LowRisk\",\"HighRisk\"], axis=1)\n",
    "# data_y is labels, data_x is features\n",
    "data_y = health_data.iloc[:, -1]\n",
    "data_x = health_data.drop(\"RiskLevel\", axis=1)\n",
    "data_x = torch.tensor(data_x.values)\n",
    "data_y = torch.tensor(data_y.values)\n",
    "\n",
    "# Split dataset into randomly split training and validation sets\n",
    "train_size = int(0.8 * len(data_x))  # 80% training\n",
    "val_size = len(data_x) - train_size  # 20% validation\n",
    "train_data, test_data = random_split(TensorDataset(data_x, data_y), [train_size, val_size])\n",
    "\n",
    "# Split into batches and give to DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# IMPLEMENT DIFFERENTIAL PRIVACY FOR MODEL\n",
    "model.train()\n",
    "privacy_engine = PrivacyEngine()\n",
    "model, optimizer, train_loader = privacy_engine.make_private(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_loader,\n",
    "    noise_multiplier=1.1,\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "best_accur = 0.0\n",
    "final_output = \"\"\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # Make sure gradient tracking is on\n",
    "    model.train(True)\n",
    "\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Clear previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        pred = model(batch_x)\n",
    "        # Compute loss\n",
    "        loss = criterion(pred, batch_y)\n",
    "        # Back propagation\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # Track loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)  # Average loss\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    # Don't track grad for testing\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            # Get prediction and calculate loss\n",
    "            pred = model(batch_x)\n",
    "            loss = criterion(pred, batch_y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Find most activated output node for each output given in tensor\n",
    "            predicted_class = torch.max(pred, dim=1)[1] \n",
    "            # Calculate accuracy\n",
    "            total += batch_x.size(0)\n",
    "            correct += (predicted_class == batch_y).float().sum()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)  # Average validation loss\n",
    "        test_accuracy = correct / total  # Validation accuracy\n",
    "\n",
    "    \n",
    "    output = f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {test_loss:.4f}, Validation Accuracy: {test_accuracy:.4f}\"\n",
    "        \n",
    "    \n",
    "    # Track best performance, and save the model's state\n",
    "    if best_accur < test_accuracy:\n",
    "        best_accur = test_accuracy\n",
    "        final_output = output\n",
    "\n",
    "print(\"BEST MODEL:\\n\"+final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64892a9d-cbb7-4383-9a50-ffdc8aeebfa1",
   "metadata": {},
   "source": [
    "# Comparing non-dp decision tree vs dp decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13536aae-afd6-4120-b21d-e1238a079368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import diffprivlib.models as dp\n",
    "\n",
    "maternal_health = pd.read_csv('maternalHealthDataSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5f42a83-f915-46a0-ab1b-0a8535549b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = maternal_health[['Age', 'SystolicBP', 'DiastolicBP', 'BS', 'BodyTemp', 'HeartRate']]\n",
    "y = maternal_health['RiskLevel']\n",
    "\n",
    "# Split data into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3203f6-0b0c-4340-802c-bd16334348e3",
   "metadata": {},
   "source": [
    "## Method 3 (Model trained with original data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "de7095bc-b636-4797-b89c-b4b284a2813d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model \n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0559f8-d987-48a8-8ee7-13447c7debbb",
   "metadata": {},
   "source": [
    "## Model 4 (DP model trained with original data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "697cc139-7a36-4ac1-a904-db99a2c9a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (0, 1, 2) # encoding of low-risk, mid-risk, high-risk\n",
    "bounds = ([], []) # TODO: Use above threshold to calculate these or just provide rough bounds\n",
    "for col in X.columns:\n",
    "    bounds[0].append(maternal_health[col].min())\n",
    "    bounds[1].append(maternal_health[col].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "df540317-dae9-4a3c-94af-379e0c516a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.61\n"
     ]
    }
   ],
   "source": [
    "dp_clf = dp.DecisionTreeClassifier(epsilon=1, bounds=bounds, classes=classes)\n",
    "dp_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = dp_clf.score(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
